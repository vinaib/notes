--------------------------------------------------------------------------------
Theory
--------------------------------------------------------------------------------
# key differentiating feature among multiprocessors architecture are mechanisms
  used to support communication among different processors:

## Message-passing architectures:
Message-passing architectures provide each processor with a local memory that is
accessible only to that processor and require processors to communicate through
explicit messages.

## Shared-memory architectures:
multiprocessors with a single address space, such as shared-memory
architectures, make the entire memory accessible to all processors and allow
processors to communicate directly through read and write operations to memory.

--------------------------------------------------------------------------------
# Advantges of Shared memory architectures:
--------------------------------------------------------------------------------
1) The single address space abstraction greatly enhances the programmability of
a multiprocessor. 
2) In comparison to a message-passing architecture, the ability of each
processor to access the entire memory simplifies programming by reducing the
need for explicit data partitioning and data movement. 
3) The single address space also provides better support for parallelizing
compilers and standard operating systems.

These factors make it substantially easier to develop and incrementally
tune parallel applications.

--------------------------------------------------------------------------------
Conceptual Model of Shared memory architecture
--------------------------------------------------------------------------------
# Called as Memory Consistency model or Memory Model
  Since shared-memory systems allow multiple processors to simultaneously read
  and write the same memory locations, programmers require a conceptual model
  for the semantics of memory operations to allow them to correctly use the
  shared memory. This model is typically referred to as a memory consistency
  model or memory model.

# Unfortunately, architecture and compiler optimizations that are required for
  efficiently supporting a single address space often complicate the memory
  behavior by causing different processors to observe distinct views of the
  shared memory.
  architecutre optimizations: Cache memory, Write buffers, Speculative reads..
--------------------------------------------------------------------------------
# Uniprocessor
--------------------------------------------------------------------------------
# Uniprocessors present a simple and intuitive view of memory to programmers

# Memory operations execute in Program order
  Memory operations are assumed to execute one at a time in the order specified
  by the program and a read is assumed to return the value of the last write to
  the same location

# Flexibility:
However, an implementation does not need to directly maintain this order among
all memory operations for correctness

# Memory operations on same location
The illusion of sequentiality can be maintained by only preserving the
sequential order among memory operations to the same location

# Memory operations on different location
## This flexibility to overlap and reorder operations to different locations is
exploited to provide efficient uniprocessor implementations.
## To hide memory latency, architectures routinely use optimizations that
overlap or pipeline memory operations and allow memory operations to complete
out-of-order.

# Summary
In summary, the uniprocessor memory model is simple and intuitive for
programmers and yet allows for high performance implementations.

--------------------------------------------------------------------------------
# Multiprocessors
--------------------------------------------------------------------------------
# Memory operations on same location
Allowing multiple processors to concurrently read and write a set of common
memory locations complicates the behavior of memory operations in a
shared-memory multiprocessor.


# Sequential Consistency (SC):
A multiprocessor system is sequentially consistent if the result of any
execution is the same as if the operations of all processors were executed in
some sequential order,and the operations of each individual processor appear in
the order specified by the program.
"This total order of operations is called memory order."

## SC: Uniprocessor
operations executed in order specified by the program

## SC: Multiprocessor
### all operations executed in order, 
### and the operations of each individual core appear in program order

Weak Ordering(WO):
-----------------
The weak ordering model (also known as weak consistency) relies on maintaining
program order only at the synchronization points in the program.
The intuition behind weak ordering is that most programs are written using
synchronization operations to coordinate memory operations on different
processors and maintaining program order at such synchronization operations
typically leads to correct outcomes (e.g., SC outcomes) for the program.

Weakly order model:
-------------------
The order of memory access is not required to be the same as the program order
for load and store operations.

Strongly order model:
---------------------
Memory transactions are guaranteed to occur in the order they are issued

Ordering requirements for Device and strongly ordered model
-----------------------------------------------------------
Address dependency:
1) An address dependency exists when the value returned by a read access is used
for the computation of the virtual address of a subsequent read or write access.

Control dependency:
1) A control dependency exists when the data value returned by a read access
determines the condition flags, and the values of the flags are used in the
condition code checking that determines the address of a subsequent read access.
This address determination might be through conditional execution, or through
the evaluation of a branch.

-----------------------------------------------------------------------
A1\A2		  |Normal access| Device access‡| Strongly-ordered access‡|
-----------------------------------------------------------------------
Normal access |		- 		|		- 		|			-			  |
-----------------------------------------------------------------------
Device access |		- 		|		< 		|			<			  |
-----------------------------------------------------------------------
Strongly-	  |				|				|						  |
ordered       | 			|				|						  |
access 		  |		- 		|		< 		|			< 			  |
-----------------------------------------------------------------------

Legend:
-------
A1/A2: instructions

"<":
# Accesses must arrive at any particular memory-mapped peripheral or block of
memory in program order, that is, A1 must arrive before A2. 

# There are no ordering restrictions about when accesses arrive at different
peripherals or blocks of memory.

"-": 
# Accesses can arrive at any memory-mapped peripheral or block of memory in
any order.

"‡" The ordering requirements for Device and Strongly-ordered accesses are
identical.

The only architecturally-required difference between Device and Strongly-ordered
memory is that:

# Strongly ordered memory:
--------------------------
a write to Strongly-ordered memory can complete only when it reaches the
peripheral or memory component accessed by the write.

# Device memory:
----------------
a write to Device memory is permitted to complete before it reaches the
peripheral or memory component accessed by the write.
