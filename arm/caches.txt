1) cache stores both address of item in main memory and its data and some status
information.
2) with caches performance is improved
3) Reduces power consumption by avoiding the need to drive external signals

--------------------------------------------------------------------------------
|31        Tag (20b)            12 | 11 set index (8b) 4 | 3 Data-index (4b) 0 |
--------------------------------------------------------------------------------

4) cahce Tag: 
The top bits of the 32-bit address tells the cache where the
information came from in main memory and is known as the tag.
The tag take up physical space in the cache.
5) size of cache:
The total cache size is a measure of the amount of data it can hold; the RAMs
used to hold tag values and status bits are not included in the calculation.

6) cache line:
It would be inefficient to hold one word of data for each tag address, so
several locations are typically grouped together under the same tag. This
logical block is commonly known as a cache line.

7) Identifying line:
The middle bits of the address, or index, identify the line.
The index is used as address for the cache RAMs and does not require storage as
a part of the tag. The index is the part of a memory address that determines in
which line(s) of the cache the address can be found.

A line refers to the smallest loadable unit of a cache, a block of contiguous
words from main memory.

The cache line contains tag, status bits and code or data.
8) offset: offset in line. Even this is not required to store in Tag RAM.
9) Associated with each line of data are one or more status bits.
instruction cache: Valid bit
Data Cache: valid bit, dirty bit
10) way:
A way is a subdivision of a cache, each way being of equal size and indexed in
the same fashion. The line associated with a particular index value from each
cache way grouped together forms a set.

11) cache controller:
-> is a hardware, that copies code or data from main memory to cache memroy
automatically. When it receives a request from the core it must check to see
whether the requested address is to be found in the cache. This is known as a
cache look-up.
-> interprets read and write memory requests before passing them on to the
memory controller. It process a request by dividing the address of the request
into three fields: tag field, set index field(line), data index field(offset).
-> controller uses "set index" portion of the address to locate the cache line
-> controller then checks valid bit to determine cache line is active
-> and compares cache tag to the tag field of the address.
-> Cache Hit/Miss:
if comparison succeeds it is hit else it is miss.
-> on cache miss: 
the request must be passed to the next level of the memory hierarchy – an L2
cache, or external memory.

12) Cache line fill:
on a cache miss, the controller copies an entire cahce line from main memory to
cahce memory and provides the requested code or data to the processor. The
copying of a cache line from memory to cache memory is known as cache line fill.

13) Data Streaming:
During cache line fill, cache controller may forward the loading data to the
coer at the same time it is copying it to cache. Streaming allows a processor to
continue execution while the cache controller fills the remaining words in the
cache line.

14) Eviction:
If valid data exits in cache line but represents another address block in main
memory, the entire cahce line is evicted and replaced by the cahce line
containing the requested address. The process of removing an existing cache line
as part of servicing cache miss is known as eviction.


Desgin facts
------------
1) larger caches for more expensive chips
2) Making an internal core cache larger can potentially limit the max speed of
the core

How cahces improves performance?
--------------------------------
1) The improvement a cache provides is possible because computer programs
execute in nonrandom ways.
2) if program's access to memory were random, a cache would provide little
improvement to overall system performance.
3) The principle of locality of reference explains the performance improvement
providede by the addition of a cache memory to a system.


principle of locality of reference
-----------------------------------
This principle states that computer software programs frequently run small loops
of code that repeatedly operate on local section of data memory. The repeated
use of the same code or data in memory is the reason a cache improves
performance.

Temporal and Spatial locality
------------------------------
The cache makes use of this repeated local reference in both time and space. 

Spatial locality:
-----------------
An access to one location is likely to be followed by accesses to adjacent
locations. Examples of this principle are:
• sequential instruction execution
• accessing a data structure.

Temporal locality:
------------------
An access to an area of memory is likely to be repeated in a short time period.
An example of this principle is the execution of a software loop.

if the reference is in time, it is called temproal locality. This means that
programs tend to reuse the same address over time.
Example: 
Code, for instance, can contain loops, meaning that the same code gets
executed repeatedly or a function can be called multiple times.

if the reference is by address, it is called spatial locality. This means tend
to use addresses that are near to each other.
Example:
Data accesses (for example, to the stack) can be limited to small regions of
memory.

Why cache line stroes multiple words?
To minimize the quantity of control information stored, the spatial locality
property groups several locations together under the same tag. This logical
block is commonly called a cache line.

-> Normally, caches are self-managing, with the updates occurring automatically

-> Different cache topologies and access policies are possible, however, they
must comply with the memory coherency model of the underlying architecture.


Write Buffers
-------------
The write buffer is a block that decouples writes being done by the core when
executing store instructions from the external memory bus. The core places the
address, control and data values associated with the store into a set of
hardware buffers. Like the cache, it sits between the core and main memory. This
enables the core to move on and execute the next instructions without having to
stop and wait for the slow main memory to actually complete the write operation.

Cache Drawbacks
---------------
1) Non deterministic execution time:
some problems that are not present in an uncached core. One such drawback is
that program execution time can become non-deterministic. 

This means that the execution time of a particular piece of code can vary
significantly. This can be something of a problem in hard real-time systems
where strongly deterministic behavior is required.

2) External devices:
You require a way to control how different parts of memory are accessed by the
cache and write buffer.
Example:
In some cases, you want the core to read up-to-date data from an external
device, such as a peripheral. It would not be sensible to use a cached value of
a timer peripheral.

3) Coherency:
The contents of cache and external memory might not be the same. this is because
the processor can update the cache contents, which have not yet been written
back to main memory.

This can be a particular problem when you have multiple cores or memory agents
like an external DMA controller.

ARM L1 Cache: Harvard Architecture: has separate caches for Instruction and
data.

ARM L2 Cache: Unified Architecture: holds both instructions and data.
ARM L2C-310 is an example of such an external L2 cache controller block.

Systems having multiple cores, will have separate L1 cache and common L2 cache.

--------------------------------------------------------------------------------
Direct Mapped Cache
--------------------------------------------------------------------------------
In a direct-mapped cache each addressed location in main memory maps to a single
location in cache memory. Since main memory is much larger than cache memory,
there are many addresses in main memory that map to the same single location in
cache memory.

-> Direct-mapped caches are subject to high levels of thrashing

Repeated cache misses result in continuous eviction of the routine that not
running. This is cache thrashing.

--------------------------------------------------------------------------------
Set Associativity: Reduce frequency of thrashing
--------------------------------------------------------------------------------
To reduce the frequency of thrashing, some caches are designed in such a way
that, cache memory is divided into smaller equal units, called ways.

The main caches of ARM cores are always implemented using a set associative
cache. improving program execution speed and giving more deterministic
execution. It comes at the cost of increased hardware complexity and a slight
increase in power(because multiple tags are compared on each cycle).

(in cortex A PG: section 8.4.3
A memory location can then map to a way rather than a line)

///  Background: start
32 bit Memory Address: [31 - 12] Tag : [11 - 4] Set index : [3 - 1]Data Index
Tag: points to memory location in main memory
Set index: points to line
Data index: offset in the line
///  Background: end

however, the set index now addresses more than one cache line—it points to one
cache line in each way. Instead of one way of 256 lines, the cache has four ways
of 64 lines. The four cache lines with the same set index are said to be in the
same set, which is the origin of the name “set index.”.

The set of cache lines pointed to by the set index are set associative. A data
or code block from main memory can be allocated to any of the four ways in a
set.

Level 2 cache implementations (such as the ARM L2C-310) can have larger numbers
of ways (higher associativity) because of their much larger size.

Cortex-A7 or Cortex-A9 processors: 
4-way set associative 32KB data cache, with an 8-word cache line length.

To find number of line in a way, then we must know number of ways, size of line
 and size of cache. As in Cortex A7 size of cache is 32KB. 
 size of line: 8 words (8 * 4 = 32 Bytes (not bits))
 Number of ways: 4
 then number of lines in each way: 32KB: (32 * 1024)/32  = 1024/4 = 256
 lines/way
 So 32 bit address:
[31 : 13] 	= tag address
[12: 5]		= 256 (number of lines)
[4:2]		= 8 (number of words]
[1:0]		= 4 (number of bytes in word)		
--------------------------------------------------------------------------------
Increasing Set Associativity: (ideal case)
--------------------------------------------------------------------------------
As the associativity of a cache controller goes up, the probability of thrashing
goes down. The ideal goal would be to maximize the set associativity of a cache
by designing it so any main memory location maps to any cache line. A cache that
does this is known as a fully associative cache.

CAM:
However, as the associativity increases, so does the complexity of the hardware
that supports it. One method used by hardware designers to increase the set
associativity of a cache includes a content addressable memory (CAM).

A CAM uses a set of comparators to compare the input tag address with a
cache-tag stored in each valid cache line.

In practice, performance improvements are minimal for Level 1 caches above 4-way
associativity, with 8-way or 16-way associativity being more useful for larger
level 2 caches.
--------------------------------------------------------------------------------
Write Buffers
--------------------------------------------------------------------------------
-> A write buffer is a very small, fast FIFO memory buffer that temporarily
holds data that the processor would normally write to main memory.

-> In a system without a write buffer, the processor writes directly to main
memory.

->The write buffer reduces the processor time taken to write small blocks of
sequential data to main memory.

->The FIFO memory of the write buffer is at the same level in the memory
hierarchy as the L1 cache.

->A write buffer also improves cache performance; the improvement occurs during
cache line evictions.

-> If the cache controller evicts a dirty cache line, it writes the cache line
to the write buffer instead of main memory.

-> The FIFO depth of a write buffer is usually quite small, only a few cache
lines deep.

Why FIFO depth is small?
Data written to the write buffer is not available for reading until it has
exited the write buffer to main memory. The same holds true for an evicted cache
line: it too cannot be read while it is in the write buffer.

Coalescing: Write Merging: Write Collapsing: Write Combining:
Some write buffers are not strictly FIFO buffers. The ARM10 family, for
example,supports coalescing—the merging of write operations into a single cache
line. The write buffer will merge the new value into an existing cache line in
the write buffer if they represent the same data block in main memory.
Coalescing is also known as write merging, write collapsing, or write combining.

sometimes the behavior of the write buffer is not what you want when accessing a
peripheral, you might want the core to stop and wait for the write to complete
before proceeding to the next step. Sometimes you really want a stream of bytes
to be written and you don’t want the stores to be combined.ARM memory ordering
model on page 10-3, describes memory types supported by the ARM architecture and
how to use these to control how caches and write buffers are used for particular
devices or parts of the memory map.

--------------------------------------------------------------------------------
Fetch Buffers
-------------------------------------------------------------------------------
->Similar components, called fetch buffers, can be used for reads.
->cores typically contain prefetch buffers that read instructions from memory
ahead of them actually being inserted into the pipeline. 
->In general,such buffers are transparent to you. 
->Some possible hazards associated with this will be overcome with memory
ordering rules.

--------------------------------------------------------------------------------
Virtual and physical tags and indexes
--------------------------------------------------------------------------------
-> Early ARM processors such as the ARM720T or ARM926EJ-S processors used
virtual addresses to provide both the index and tag values. This has the
advantage that the core can do a cache look-up without the need for a virtual to
physical address translation.

-> The drawback is that changing the virtual to physical mappings in the system
means that the cache must first be cleaned and invalidated, and this can have a
significant performance impact.

-> ARM11 family processors use a different cache tag scheme. Here, the cache
index is still derived from a virtual address, but the tag is taken from the
physical address. The advantage of a physical tagging scheme is that changes in
virtual to physical mappings do not now require the cache to be invalidated.
This can have significant benefits for complex multi-tasking operating systems
that can frequently modify translation table mappings.

-> It means that the cache hardware can read the tag value from the appropriate
line in each way in parallel without actually performing the virtual to physical
address translation, giving a fast cache response. Such a cache is often
described as Virtually Indexed, Physically Tagged (VIPT).

Page Coloring:
-> there is a drawback to a VIPT implementation. For a 4-way set associative
32KB or 64KB cache, bits [12] and [13] of the address are required to select the
index. If 4KB pages are used in the MMU, bits [13:12] of the virtual address
might not be equal to bits [13:12] of the physical address. There is therefore
scope for potential cache coherency problems if multiple virtual address
mappings point to the same physical address.

This is solved by page coloring abd exists on other processor architectures too

This problem is avoided by using a Physically Indexed, Physically Tagged (PIPT)
cache implementation. The Cortex-A series of processors use such a scheme
for their data caches. It means that page coloring issues are avoided, but
at the cost of hardware complexity.

--------------------------------------------------------------------------------
Cache Policies
--------------------------------------------------------------------------------
-> Allocation policy:

Read Allocate Policy:
A read allocate policy allocates a cache line only on a read. If a write is
performed by the core that misses in the cache, the cache is not affected and
the write goes to the next level of the hierarchy.

write allocate policy:
allocates a cache line for either a read or write that misses in the cache (and
so might more accurately be called a read-write cache allocate policy).

This is typically used in combination with a write-back write policy on current
ARM cores.

-> Replacement policy: (Victim, eviction)

When there is a cache miss, the cache controller must select one of the cache
lines in the set for the incoming data. The cache line selected is called the
victim. If the victim contains valid, dirty data, the contents of that line must
be written to main memory before new data can be written to the victim cache
line. This is called eviction.

The replacement policy is what controls the victim selection process.

Round-Robin:
Round-robin or cyclic replacement means that you have a counter (the victim
counter) that cycles through the available ways and cycles back to 0
when it reaches the maximum number of ways.

Pseudo-Random:
Pseudo-random replacement randomly selects the next cache line in a set to
replace. The victim counter is incremented in a pseudo-random fashion and can
point to any line in the set.

Least Recently Used(LRU):
Least Recently Used (LRU) replacement is used to replace the cache line or page
that was least recently used.

Most ARM processors support both Round-robin and Pseudo random policies. 
The Cortex-A15 processor also supports LRU.

A round-robin replacement policy is generally more predictable, but can suffer
from poor performance in certain use cases and for this reason, 
the pseudo-random policy is often preferred.

Write policy:
When the core executes a store instruction, a cache lookup on the address(es) to
be written is performed. For a cache hit on a write, there are two choices.

Write-through:
->With this policy writes are performed to both the cache and main memory.
->This means that the cache and main memory are kept coherent.

Write-back:
->In this case, writes are performed only to the cache, and not to main memory.
->This means that cache lines and main memory can contain different data.
->The cache line holds newer data, and main memory contains older data 
(said to be stale). 
->To mark these lines, each line of the cache has an associated dirty bit 
(or bits). 
->When a write happens that updates the cache, but not main memory, 
the dirty bit is set. 
->If the cache later evicts a cache line whose dirty bit is set (a dirty line),
it writes the line out to main memory. Using a write-back cache policy can
significantly reduce traffic to slow external memory and therefore improve
performance and save power.
->if there are other agents in the system that can access memory at the same
time as the core, you must consider coherency issues.

--------------------------------------------------------------------------------
Cache performance and hit rate
--------------------------------------------------------------------------------
The hit rate is defined as the number of cache hits divided by the number of
memory requests made to the cache during a specified time, normally calculated
as a percentage.

the miss rate is the number of total cache misses divided by the total number of
memory requests made to the cache.

Clearly, a higher hit rate will generally result in higher performance.

There are some simple rules that can be followed to give better performance.

-> The most obvious of these is to enable caches and write buffers and to use
them wherever possible (typically for all parts of the memory system that
contain code and more generally for RAM and ROM, but not peripherals).

-> Performance will be considerably increased in Cortex-A series processors if
instruction memory is cached.

-> Placing frequently accessed data together in memory can also be helpful. For
example, a frequently accessed array might benefit from having a base address at
the start of a cache line.

cache-friendly:
Fetching a data value in memory involves fetching a whole cache line; if none of
the other words in the cache line will be used, there will be little or no
performance gain.

This can be mitigated by accesses to sequential addresses, for example,
accessing a row of an array, benefit from cache behavior.

Non-predictable or non-sequential access patterns, for example, linked lists, do
not.

Smaller code might cache better than larger code and this can sometimes give
seemingly paradoxical results.

For example, a piece of C code might fit entirely within cache when compiled for
Thumb (or for the smallest size) but not when compiled for ARM (or for maximum
performance) and as a consequence can actually run faster than the more
optimized version.

--------------------------------------------------------------------------------
Invalidating and cleaning cache memory
--------------------------------------------------------------------------------
When is it required?
Cleaning and invalidation can be required when the contents of external memory
have been changed and you want to remove stale data from the cache.

It can also be required after MMU related activity such as changing access
permissions, cache policies, or virtual to physical address mappings.

Invalidation:
-> Invalidation of a cache or cache line means to clear it of data.
-> This is done by clearing the valid bit of one or more cache lines.
-> he cache must always be invalidated after reset as its contents will be
undefined.
-> On invalidate, any updated data in the cache from writes to write-back
cacheable regions would be lost by simple invalidation.

Cleaning:
-> Cleaning a cache means writing the contents of dirty cache lines out to main
memory
-> and Clear the dirty bit in cache line
-> Cleaning cache makes the contents of the cache line and main memory coherent
with each other.
-> This is only applicable for data caches in which a write-back policy is used
-> Cache invalidate, and cache clean operations can be performed by cache set,
	or way, or by virtual address.

CP15 operations:	
-> The commands to either clean or invalidate the cache are CP15 operations.	
-> They are available only to privileged code and cannot be executed in User
 mode.
-> CP15 instructions exist that will clean, invalidate level 1 and level 2 data
   and instruction caches.

--------------------------------------------------------------------------------
Point of Coherency(PoC): External Memory
--------------------------------------------------------------------------------
For a particular address, the PoC is the point at which all blocks, for example,
cores, DSPs, or DMA engines, that can access memory are guaranteed to see the
same copy of a memory location. Typically, this will be the main external system
memory

--------------------------------------------------------------------------------
Point of Unification(PoU): External Memory/L2
--------------------------------------------------------------------------------
The PoU for a core is the point at which the instruction and data caches of the
core are guaranteed to see the same copy of a memory location. For example, a 
unified level 2 cache would be the point of unification in a system with Harvard 
level 1 caches and a TLB for cacheing translation table entries. If no external
cache is present, main memory would be the Point of unification.

In the Cortex-A9 processor the PoC and PoU is essentially the same place, at the
L2 interface.


